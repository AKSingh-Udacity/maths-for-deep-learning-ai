[
["index.html", "Mathematics for Deep Learning and Artificial Intelligence Preface", " Mathematics for Deep Learning and Artificial Intelligence Christian Ramsey &amp; Haohan Wang 2018-11-01 Preface Our new book "],
["intro.html", "1 Introduction to Artificial Intelligence 1.1 What is it to think? 1.2 Reasoning and Logic 1.3 Dealing with Uncertainty 1.4 The Neurobiology and Cognitive Science 1.5 Representions and the Rise of Pattern Recognition 1.6 The Mathematics Needed", " 1 Introduction to Artificial Intelligence Some significant applications are demonstrated in this chapter. ..draft.7.. 1.1 What is it to think? Artificial intelligence is harder to describe in finite terms than it is to recreate many of the popular algorithms that serve to represent it. This is because AI has always been fundamentally tied up with some of our species deepest philosophical queries, possibly more than any other discipline. We ask ourselves many questions such as: What does it mean to be human? What makes us different or similar? What is the difference between humans and other animals? Is there another species that is more intelligent than we are (Übermensch)? Will humans be surpassed by a more intelligent species? The question as to whether these questions are valid or not is beyond the scope of this book, but the questions themselves do provide us with a view into the history of artificial intelligence. Artificial intelligence is by nature, multidisciplinary; logicians, mathematicians, neuroscientists, theologians, philosophers, computer scientists, statisticians, ethicists, roboticist, cognitive psychologists, ethnologists, are just a few of the titles of those who have made AI what it is today. The design of intelligent machines has always been serious business. For many the question starts with “what is thinking and reasoning? What is it to think and reason well?” and even before the invention of computers there were questions such as “Can correct reasoning be mechanised?”. These are just some of questions that the likes of Aristotle, Leibienitz, Turing, Laplace, Wittengenstein, Godel, and Boole attempted to answer definitively. The second set of questions were just as philosophically motivated, but instead from different disiplines like biology, espeically neurobiology, then called neurophysiology which took on different questions. As we began to understand an infinitesimal amount about how very basic elements called neurons can give rise to thinking and intelligence, they too had similar questions, “Can machines think?”, “Can the brain be emulated?”. These questions moved away from deductive reasoning and towards emulating our senses and pereptions. Rosenblatt, Hebb, Minsky, and others were able to move towards learning from data as humans do by recognising patterns which birthed the first neural network, the perceptron. This dichotomy, between symbolism and perception, like are dichotomies don’t pin down the reality but we hope that it provides a way of thinking abbout our path towards today’s artificial intelligence. We are now at a stage where we are bringing what we have learned in symbolic. 1.2 Reasoning and Logic Rene Descartes stated “Cogito, ergo sum” which translated means “I think therefore I am”. He arrived at this statement after trying to use pure “reason” to find what was fundamentally true without any doubt. Although he couldn’t complete his project, his value of “reason” was not unpopular and is gaining popularity in much of modern culture and can be heard in everyday conversations when we talk about the distinction between our thoughts and body. Reasoning starts with logic. Aristotle. So let us begin with reasoning. The branch and discipline titled logic was originally focused on finding absolute truths from statements that could be found in everyday discourse. Logic is fundamentally tied up with questions around how one can reason correctly. How one is able to deduce the correct answer given a set of statement or propositions. What is the connection between logic and AI in history and maths? Difference between propositional logic and symbolic logic Where do we use logic in deep learning Where does boolean logic end up ? In designing circuits From propositional logic to boolean algebra From Aristotle to George Boole to Claude Shannon Books Mathematical for Analysis of Logic An Investigation of the Laws of Thought Propositions can be TRUE or FALSE Propositions can be compounded 1.3 Dealing with Uncertainty 1.3.1 Probability 1.3.2 Information Theory 1.4 The Neurobiology and Cognitive Science 1.4.1 Cortexes 1.4.2 Neurons 1.4.3 Perceptrons and Perception 1.5 Representions and the Rise of Pattern Recognition 1.6 The Mathematics Needed 1.6.1 Logic 1.6.2 Calculus 1.6.3 Probability 1.6.4 Geometry 1.6.5 Information Theory 1.6.6 Statistical Learning Theory 1.6.7 Representation Learning 1.6.8 Optimization 1.6.9 Graph Theory (not sure we will cover this) 1.6.10 Reinforcement Learning (not sure we will cover this) "],
["into-to-logic-and-reasoning.html", "2 Into to Logic and Reasoning 2.1 History of Logic 2.2 Elements of Logic 2.3 Godel and Turing’s theorems", " 2 Into to Logic and Reasoning 2.1 History of Logic As we all know that Artificial Intelligence (‘AI’) is the field of computer science that was developed to enable computers/machines to display behavior that can be characterized as intelligent. Then you may ask “How should we define intelligence? Why are we human intelligent?” As we go through the history of logic later, you will see how this the search of this question has started thousand years. Based on this journey of ‘intelligent’ search, we can now shed some light on how and why ai was developed. We human have long been regarded as intelligent due to our ability to think or reason. And this is the exact start of logic. First, let’s begin the journey together to uncover the attempt that we human took to discover intelligence. Logic was developed by Aristotle (384-322 BCE). He introduced the formal study of what is now known as ‘formal logic’, the logic that is concerned with the form, not the content, of statements or propositions. Basically, Aristotles system of logic introduced hypothetical syllogism, temporal model logic and the inductive logic. He holds that a propositon is a complex involving 2 terms, a subject and a predicate. Each of them are represented with a noun. The logic form of a proposition is determined by its quantity and by its quality. All Aristotle’s logic revolves around one notion: the deduction (syllogism). Aristotle says: &quot; A deduction is speech in which, certain things having been supposed, somehing different from those supposed results of necessity because of their being so.&quot; Each of the ‘things supposed’ is a premise of the argument and what ‘results of necessity’ is the conclusion. Following Greek’s tradition in logic, Cicero (106-43 BCE) introduced the term ‘proposition’ for which we will discuss in depth later. Alexander of Aphrodisias (3rd century A.D.) used the term ‘logic’ in the modern sense of distinguishing correct from incorrect reasoning. In the early twelfth centure, Peter Abelard wrote extensive commentaries attempting to articulate issues like opposition, conversion, opposition, quantity, and quality, and composed his treatise, ‘the Dialectica’. Later, William of Sherwood developed mnemonic verse as an aid in mastering the syllogisms. Jean Buridan elaborated a theory of conseqences which somewhere discussed the rules of inference. Around the same time, Scholastic logician Raymond Lully used logic to prove the Christian faith. He also remarkably designed machines that would perform logical calculations, and is thus arguably considered as the father of computer programming. 2.1.1 Medieval Logic Medieval Logic generally means the form of Aristotelian logic developed in medieval Occident throughout the period c. 1200-1600. 2.1.2 Traditional Logic It starts with Antonine Arnauld and Pierre Nicole’s Logic, or the Art of Thinking published in 1662 or the Port Royal Logic in which the logic is defined as the ‘art of managing one’s reason right in the knowledge of things, both for the instruction of oneself and of others’. It was the most influential work on logic in England until John Stuart Mill’s System of Logic in 1825. Having invented calculus, Wilhelm Leibniz (1646-1716) concluded that the whole of logic actually depends on mathematics and thus worked on reducing scientific and philosophical speculation to computation. He also suggested that a unversial calculus of reasoning could be devised which would provide an automatic method of solution for all problems which could be expressed in the universal language. The current understanding of the power of Leibniz’s discoveries did not emerge until the 1980s. 2.1.3 Modern and Contemporary Period (1850 - Present) During this period, logicians in the modern period ‘rediscovered’ the Stoic logic of proposition. People like Augustus De Morgan (1806-1864) proposed some theorem in that logic which now bear his name. Considered the founder of symbolic logic and Boolean Algebra, which is the basis of all modern computer arithmetic. George Boole (1815-1864) gave us Boolean Logic which treats propositions as either true or false. His use of numbers to express the truth values of compound statements have significantly influenced the development of computers and he is regarded as being one of the founders of the field of computer science. Until today, programmers still use his principle to test the trugh of program results or user feedback. John Venn (1834-1923) was a Cambridge logician who published 3 standard texts in logic, The Logic of Chance 1866, symbolic Logic 1881, and The Principles of Empirical Logic 1889. He is remembered for introducing the circular diagrams as a tool to test the validity of syllogisms, known as Venn Diagrams. Around the same time, John Stuart Mill (1806-1873) made a thorough study about inductive reasoning and introduced methods for checking such arguments now known as ‘Mill’s Methods’. And Charles Sanders Peirce (1839-1914) introduced Pragmatism, at the core of which he argued that ideas should be evaluated solely by their practical effects and not by any intrinsic qualities of reason or logic. Gottlob Frege (1848-1925) pronounced that logic is the basis of methematics and that arithmetic and analysis are part of logic. He has been called the greatest logician since Aristotle. By developing the predicate calculus (Quantification Theory), he had combined Aristotelian and Stoic’s logics and his work was the foundation or beginning point for an enormous outpouring of work in formal logic. In 1903, Bertrand Russell (1872-1970) started his project ‘The Principles of Mathematics’ in which he purposed to prove that ‘all pure mathematics deals exclusively with concepts definable in terms of a very small number of logic principles.’ Russel also continued the development of the predicate calculus and he alos found the inconsistency in Ferge’s system (because Russell’s Paradox could be derived within Frege’s system). The next wave of the logic can be called the ‘mathematical school’. This tradition or school includes the work of Richard Dedekind (1831-1916), Giuseppe Peano (1858-1932), David Hibert (1862-1943). Ernst Zermelo (1871-1953), and many ohter since then. Its goal was the axiomatization of particular branches of mathematics, including geometry, arithetic, analysis, and set theory. In 1889 Peano published the first version of the logical axiomatization of arithmetic. Five of the nine axioms he came up with are now known as the Peano axioms. One of these axioms was a formalized statement of the principle of mathematical induction. Ernst Zermelo’s axiomatic set theory was also an attempt to escape Russell’s Paradox. His axioms went well beyond Frege’s axioms of extensionality and unlimited set abstraction, and evolved into the now-canonical Zermelo-Fraenkel set theory. Gradually, logic became the branch of mathematics that was to be brought within the axiomatic methodology. Jan Lukasiewicz worked on multi-valued logics. His three-valued propositional calculus, introduced in 1917, was the first explicitly axiomatized non-classical logical calculus. The famous Ludwig Wittgenstein (1819-1951) entered the list of significant logician by being oen of the developers of the ‘truth tables’. This intensive work on mathematical issues culminated in the work of Kurt Godel (1906-1978), a logician of the caliber of Aristotle and Frege. Using many applications of the rules of logic, Kurt Godel proved his ‘incompleteness theorem’, which proposes that some parts of mathematics are based on ideas that cannot be proved within the system of mathematics. Godel was also one of the central figures in the study of computability. Others included Alonzo Church (1903-1995), Alan Turing (1912-1954), and others. Let’s pause here for a while and take a close look at the progress that made by Alan Turing and Kurt Godel during this period. As we all know that, Alan Turing, the father of computing, created a machine that can accept different instructions for different tasks in 1936 and marked the first step of the AI with his seminal 1950 paper. In the paper, he introduced the Turing test to determine whether a computer can be regarde intelligent. Turing’s initial investigation of computation stemmed from the programme set out by David Hilbert in 1928. Hibert presented 3 open questions for logi and mathematics. Was mathematics complete in a sense that any mathematical assertion could either be proved or disproval consistent in the sense that false statements could not be drived by a sequence of valid steps and decidable in the sense that there exists a definite method to decide the truth of falsity of every mathematical assertion. Within 3 years, Kurt Godel had proved that the axioms of arithematic are both not complete and consistent. By 1937 both Alonzo Church and Alan Turing had demonstrated that undecidability of particular mathematical assertions. Interestingly, as Godel and Church had depended on demonstrating their results using purely mathematical calculi. Turing chose to take an unusual route of considering mathematical proof as an artifact of human reasoning. He even generalized this notion to a physical machine that he believes it could emulate a human mathematician and in turn there could be a universal machine that can emulate all other computing machines. Then he used this construct to show that certain functions cannot be computed by such a universal machine and in turn, demonstrated the undecidability of assertions associated with such functions. As we can easily tell, at the heart of Turing’s universl machine is a model of human reasoning and calculation. After putting his idea into the practice during the second World War, he came up with a comprehensive paper that provided a philosophical framework for answering the question ‘Can machine think?’ i.e. the invention of the ‘Turing test’ and universality of digital computers. He also goes on to discuss 2 distinct strategies that might be considered possible of achieving a thinking machine: AI by programming Ai by machine learning AI using logic, probabilities, learning and background knowledge There are plenty of other advances in logic afterwards as well. Logical empiricist Rudolph Carnap (1891-1970) was associated with the famous verfiability principle, according to which a synthetic statement is meaninful only if it is verificable. Logical positivist A.J. Ayer, on the other hand, wrote in 1936 his ‘Language, Truth, and Logic’ in which he focused on the role of language as the medium through which knowledge is understood and verified. In 1965, Lofti Zadeh developed ‘fuzzy logic’ which allows imprecise answers to questions in addition to being either clear-cut true or false. This logic now serves as the basis of computer programming designed to mimic human intelligence. Now let’s go back to our initial question about AI. As we can see from the above journey, AI has been heavily influenced by some of these logical ideas and undoubtedly, logic has played an crutial role in some central areas of AI research. As much as we want to me Logic in AI currently is still a large and rapidly growing field. One of the most influential figure in logical AI is John MccCarthy. McCarthy was also one of the founders of AI, and consistenly advocated a research methodology that uses logical techniques to formalize the reasoning problems that AI needs to solve. You may wonder, what motivated them to continue integrating logic with AI. The motivation for using logic is that a logical formalization helps us to understand the reasoning problem itself. However, as we currently know, the key to solve a problem may not need an thorough understanding of what the reasoning problems are. It is in fact quite controversial in the context of AI, an alternative methodology would seek to learn or evolve the desired behaviors. Knowledge representation and reasoning: representing information about the world in a form that a machine can utilize to solve complex tasks. It was inspired by psychology and neurobiology about how humans solve problems and represent knowledge in order to design formalisms that can make a complex concept easier o learn and design. Actually, knowledge representation starts from logic and then moved to automate various kinds of reasoning i.e. the application of rules or the relations of sets and subsets. 2.1.4 The End of Logic? The intellectual project to capture thinking in terms of logic was at first thought to solve the problem of reasoning only to find out later that what makes human intelligent isn’t just reasoning, but reasoning under uncertain conditions as well as learning not just from rules but from the world itself. So it is not the end of logic, for it powers so much of modern computing, but like any grand theory that takes on something as complex as human level thinking it has found it’s practical place as a tool forever in our intellectual and computational toolbelt. 2.2 Elements of Logic 2.2.1 Propositional Logic 2.2.2 Operators 2.2.3 Definitions 2.2.4 First-Order/Predicate Logic 2.2.5 Boolean Algebra 2.3 Godel and Turing’s theorems "]
]
