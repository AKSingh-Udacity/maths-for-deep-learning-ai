---
title: "hw_intro"
author: "hw"
date: "December 3, 2018"
output: html_document
---

# Intro 
## What Makes us Human

As Aristotle said, the essence of a man is to know and to desire to know. Man, by his rational intellect, is meant to seek truth and to understand the reality in which he exists. Aristotle regards our intellect as the way to define human. It seems like a very appealing definition. But is that all what makes us human?

Some may argue, humans have mind. Mind consists of intellect and will. In a sense, the intellect is knowledge and the will is active choice. We human have mind and our free will to choose. You may also argue, we are special because our ability to create and to malnipulate symbols i.e. language. Our language and memory allow us to create and maintain the culture from one generation to another and this is what meant to be a human. 

Moreover, some may also consider the fact that we are distinguished by our awareness of our own death. 

You are all right. All of these make us what we are - a human being. 
======
Some may argue, humans have mind. Mind consists of intellect and will. In a sense, the intellect is knowledge and the will is active choice. We human have mind and our free will to choose. In other words, we human seems to have the ability to distinguish right from wrong and we have an adequate mental model that helps us navigate through the world and support our decision making. We are consitent learning and interacting with the external world which consintuous help us to be better adaptive to the world. 

You may also argue, we are special because our ability to create and to malnipulate symbols i.e. language. Our language and memory allow us to create and maintain the culture from one generation to another and this is what meant to be a human. It could go back to the neurobiology root of human brain. We human have sophisicated brain and mind.  


# Thinking and Reasoning 
## Logic 
Logic and AI has a long and interwined history. As the one of the first steps we human took to uncover the truth of being a human, many scientists especially logicians have came to the same conclution that what makes human unique is our ability to think and reason. Logic was developed as the main tool to support this goal. 

Dating back to 300 BC, their were major figures across many the field of philosophy, sociology, economics, and mathematics have looked to reasoning as one of the key distinctions of humankind. Our ability to take a set of propositions and to know their truth and false values. Only from this base of propositions can we begin to conclude or make logical combinations to create valid knowledge.

%connect peoplewith the otherperson not just thelist of events

[Aristotle](https://en.wikipedia.org/wiki/Aristotle) (384-322 BCE), as one of the first pioneers in developing the subject of logic, he set out to conduct the formal study of what is now known as 'formal logic'. He called humans "animal rationale", in his treatise, 

%On the Soul,  who saw humans as distinguised by their rationality, proposed the syllogism; Which was one of the first formulations of logic around 300 B.C. found in the Organon.

 In 1275, Ramon Llull, a spanish Theologian wrote the Ars Magna (Art of Finding Truth), which provided a method based in logic to produce new knowledge.
 
 %connectedback to labniz
 
Followed their footstep, Rene Descartes also came up with one of the best-known quotations in philosophy:"Cogito, ergo sum" ("I think, therefore I am"). Besides being a 'rational' philosopher, his contribution to mathematics was also of the first order, he is the inventor of Cartesian coordinate system and the founder of analytics geometry which has laid the foundation of the later developed calculus and mathematical analysis.=

%Charles Babbage 

%add the even pf rene insteadof thesummary 

[Gottfried Wilhelm Leibniz](https://en.wikipedia.org/wiki/Gottfried_Wilhelm_Leibniz) (1646-1716), along with Rene Descartes, was a major figure in the Continental Rationalism movement. His contributions to logic were perhaps the most important between Aristotle and the later extended model symbolic logic. As the inventor of calculus, and as the discoverer of the binary system. He suggested that a universal calculus of reasoning could be devised which would provide an automatic method of solution for all problems which could be expressinged in the universal language.


%add tje principle of mathematics 

Symbolic logic was not fully developed until the invention of Boolean Algebra. As the inventor of the Boolean Algebra, [George Boole](https://en.wikipedia.org/wiki/George_Boole) (1815-1864) is also a philosopher and logician who proposed that:
> No general method for the solution of questions in the theory of probabilities can be established which does not explicitly recognise, not only the special numerical bases of the science, but also those universal laws of thought which are the basis of all reasoning, and which, whatever they may be as to their essence, are at least mathematical as to their form.

After George Boole, plenty of other well-known logician and mathematicians joined the force to push the development of logic and helped to establish the unshakable status of logic as one of the most important properties of human. 

%discovery vs Inventiom

[Gottlob Frege](https://en.wikipedia.org/wiki/Gottlob_Frege) (1848-1925) %discovered predicate logic. He is well-acknowledged by many to be the father of analytic philosophy, concentrating on the philosophy of language and mathematics. His work was later brought to the center of the attention by Giuseppe Peano (1858–1932) and Bertrand Russell (1872–1970) and served as the beginning point for an enormous outpouring of work in formal logic. 

As the inventor of Turing machine and famous Turing test, [Alan Turing](https://en.wikipedia.org/wiki/Alan_Turing) is widely considered to be the father of theoretical computer science and artificial intelligence. The Turing machine that he designed is a mathematical model of computation which is capable of simulating that algorithm's logic can be constructed.

%rewrite the alanturing story and events and highly correlatedwith logic 

Claude Shannon (1916-2001) is noted for having founded information theory with a landmark paper, A Mathematical Theory of Communication, that he published in 1948. He is, perhaps, equally well known for founding digital circuit design theory in 1937, when—as a 21-year-old master's degree student at the Massachusetts Institute of Technology (MIT)—he wrote his thesis demonstrating that electrical applications of Boolean algebra could construct any logical numerical relationship

All of those people have brought logic to life and step by step helped the invention of the early computer and the birth of AI. 

## Reason Under Uncertainty 

Logic, as the proxy to human reasoning and thinking, gave us a way to reason under specific conditions, those conditions didn't match the typical type of problem a human comes into contact with in the real world. As we orient ourselves towards concrete and abstract goals we often have to settle with an incomplete picture, lacking the certainty of propositions that logic requires. So we needed a way to reason under uncertainty, this brought on the theory of probabilities. 

> The true logic of this world is to be found in theory of probability.
  - James Clark Maxwell

By definition, probabilistic reasoning is to combine the capacity of probability theory to handle uncertainty with the capacity of deductive logic to exploit structure of formal argument. Probability theory itself as the branch of mathematics could be considered as the tool that helps us reason under uncertainty. 

The start of probability really dates back to 1550 by Cardan, but wouldn't be of practical use until the correspondence of Pierre de Fermat (1607-1665) and Blaise Pascal (1623-1662) in 1654. 

Pascal was an important mathematician, helping create two major new areas of research: he wrote a significant treatise on the subject of projective geometry at the age of 16. Besides that, he also started some pioneering work on calculating machines when was still a teenager in 1642. After 3 years of effoft and 50 prototypes, he built 20 finished machines called Pascal's calculators, eastablishing him as one of the first 2 inventors ofthe machanical calculator. 

Together with René Descartes, Fermat was one of the two leading mathematicians of the first half of the 17th century. He has pioneered the work in determining maxima, minima, and tangents to various curves that was equivalent to differential calculus in his work Methodus ad disquirendam maximam et minimam and in De tangentibus linearum curvarum published in 1636.
 
Pascal and Fermet were discussing the problem that Chevalier de Méré proposed to Pascal, titled the problem of points. 

The game went as follows:

Suppose two players who have equal chance of wining each round. Their is a prize pot where they have both contributed 50% each. The game has a priori number of rounds and who every wins the most rounds takes the prize pot. But let us suppose that the game is interupptd before the end of the game. How does one divide the pot fairly?

There were others how attempted to solve this problem but the current outcomes lead to edge cases that were unintuitive and were mostly based on the past games.

Pascal and Ferment made a breakthrough by considering (from the interruption forward) the possible futures that might be possible if the players had continued until the end. 

$$ r = rounds\ for\ p1\ to\ win $$
$$ s = rounds\ for\ p2\ to\ win $$
 $$ r + s - 1 $$
 
 $$ Number\ of\ possible\ outcomes\ 2^{r+s-1} $$ 
 Odds vs. expectations
 Soon after Pascale improving on Ferment's results, Christiaan Huygens, used Pascal's result to create the first treatise of modern Probability based on expectation.s
 
> For example, the probability of throwing a 6 on a die twice is 1⁄6 x 1⁄6 = 1⁄36 ("and" works like multiplication); the probability of throwing either a 3 or a 6 is 1⁄6 + 1⁄6 = 1⁄3 ("or" works like addition).
 
 This formalisation would lead to the foundation of modern probability. 
 
 The work done by Fermat and Pascal into the calculus of probabilities laid important groundwork for Leibniz' formulation of the calculus. They are now regarded as joint founders of probability theory.
 
  Later Pascal would go on to use probability in his most notable work, Pensees, used a probabilistic argument, Pascal's Wager, where he made a convincing argument to justify belief in God and a virtuous life. 
 
 Around the same time, Christiaan Huygens (1629-1695) also started his work in probability and wrote his first treatise on probability theory in 1657 with the work Van Rekeningh in Spelen van Gluck. Frans van Schooten translated the work as De ratiociniis in ludo aleae ("On Reasoning in Games of Chance"). The work is a systematic treatise on probability and deals with games of chance and in particular the problem of points.
 
 The modern concept of probability grew out of the use of expectation values by Huygens and Blaise Pascal.  
 
 These probability pioneers has established the method that is now called *classical approach* to compute probabilities.
 
A few decades later Jacob Bernoulli (1654-1705) proved that the frequency method and the classical method are consistent with one another in his book Ars Conjectandi in 1713. His significant contributions to probability also include the derivation of the first version of the *law of large numbers*  in his work Ars Conjectandi and the start of some early work on the probability inference. 
 
 However, *classical approach* is limited by its assumption that the possibility of all outcomes of an event has to be equal.

 Nearly a centruy later, following the work of Fermet, Pascal and Huygens, Abraham de Moivre (1667-1754) gave the first statement of the formula for the normal distribution. De Moivre provided many tools to make the classical method more useful, including the multiplication rule, in his book The Doctrine of Chances in 1718 which is said to have been prized by gamblers. De Moivre first discovered Binet's formula, the closed-form expression for Fibonacci numbers linking the nth power of the golden ratio φ to the nth Fibonacci number. He also was the first to postulate the central limit theorem, a cornerstone of probability theory.

By then, the probability has gradually moved from games of chance to a scientific subject. A new era of probability was began from 18th century. 

As one of the leading figures in formulating Bayes' Theorem, Thomas Bayes (1701-1761) proposed the unique solution to a problem of inverse probability that was presented in "An Essay towards solving a Problem in the Doctrine of Chances" which was read to the Royal Society in 1763 after Bayes' death. This specific case of the probability theorem bears his name: Bayes' Theorem, however, Bayes himself may not have embraced the broad interpretation now called Bayesian, which was in fact pioneered and popularized by Pierre-Simon Laplace. 

As mentioned earlier, the Bayesian interpretation of probability was developed mainly by Laplace. Pierre-Simon Laplace (1749-1827) presented a mathematical theory of probability with an emphasis on scientific applications in his 1812 book Theorie Analytique des Probabilities, where he elucidates the use of probablity theory to scientific and practical problems in statistical mechanics, actuarial mathematics, error, and so on.

However, Laplace has only considered the classical method, leaving no indication on how the method was to be applied to general problems.

 
 Probability became a general workhorse. Andrey Andreyevich Markov introduced the Markov chains in 1906 when he produced the first theoretical results for stochastic processes by using the term “chain” for the first time. In 1913 he calculated letter sequences of the Russian language. 
 
 A generalization to countable infinite state spaces was given by Kolmogorov (1931). Markov chains are related to Brownian motion and the ergodic hypothesis, two topics in physics which were important in the early years of the twentieth century. But Markov appears to have pursued this out of a mathematical motivation, namely the extension of the law of large numbers to dependent events. Out of this approach grew a general stw a general statistical instrument, the so-called stochastic Markov process. 

Andrey also developed the first rigorous approach to probability in his 1933 monograph Grundbegriffe der Wahrscheinlichkeitsrechnun. His pioneering work, About the Analytical Methods of Probability Theory, was published (in German) in 1931. In 1933, Kolmogorov published his book, Foundations of the Theory of Probability, in which he combined the notion of sample space, introduced by Richard von Mises, and measure theory and presented his axiom system for probability theory in his book. This became the mostly undisputed axiomatic basis for modern probability theory.

ET Jaynes (1922-1998) wrote extensively on statistical mechanics and on foundations of probability and statistical inference, initiating in 1957 the maximum entropy interpretation of thermodynamics as being a particular application of more general Bayesian/information theory techniques. Jaynes strongly promoted the interpretation of probability theory as an extension of logic. His posthumous book, Probability Theory: The Logic of Science (2003) gathers various threads of modern thinking about Bayesian probability and statistical inference, develops the notion of probability theory as extended logic, and contrasts the advantages of Bayesian techniques with the results of other approaches. 

Judea Pearl (1936-) is best known for championing the probabilistic approach to artificial intelligence and the development of Bayesian networks. He is the 2011 winner of the ACM Turing Award, the highest distinction in computer science, "for fundamental contributions to artificial intelligence through the development of a calculus for probabilistic and causal reasoning". His book Probabilistic Reasoning in Intelligent Systems published in 1988 is a complete and accessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. 

Probability theory was initially derived from the gambling puzzles and graduadully became our most handy tools to reason under uncertainty. As the extention of logic, probability also maintains the properties that are at odds of deductive logic and should be considered as one of most important tools under our reasoning tool belt, 


**Optional**
Josiah Willard Gibbs (1839-1903) together with James Clerk Maxwell and Ludwig Boltzmann created statistical mechanics (a term that he coined), explaining the laws of thermodynamics as consequences of the statistical properties of ensembles of the possible states of a physical system composed of many particles. Gibbs's derivation of the laws of thermodynamics from the statistical properties of systems consisting of many particles was presented in his highly influential textbook Elementary Principles in Statistical Mechanics, published in 1902.
