---
title: Intro + Logic Reasoning
description: |
  Edited version of logic
date: "`r Sys.Date()`"
output: radix::radix_article
---

# Intro 
## What Makes us Human

As Aristotle said, the essence of a man is to know and to desire to know. Man, by his rational intellect, is meant to seek truth and to understand the reality in which he exists. Aristotle regards our intellect as the way to define human. It seems like a very appealing definition. But is that all what makes us human?

Some may argue, humans have mind. Mind consists of intellect and will. In a sense, the intellect is knowledge and the will is active choice. We human have mind and our free will to choose. You may also argue, we are special because our ability to create and to malnipulate symbols i.e. language. Our language and memory allow us to create and maintain the culture from one generation to another and this is what meant to be a human. 

Moreover, some may also consider the fact that we are distinguished by our awareness of our own death. 

You are all right. All of these make us what we are - a human being. 
Some may argue, humans have mind. Mind consists of intellect and will. In a sense, the intellect is knowledge and the will is active choice. We human have mind and our free will to choose. In other words, we human seems to have the ability to distinguish right from wrong and we have an adequate mental model that helps us navigate through the world and support our decision making. We are consitent learning and interacting with the external world which consintuous help us to be better adaptive to the world. 

You may also argue, we are special because our ability to create and to malnipulate symbols i.e. language. Our language and memory allow us to create and maintain the culture from one generation to another and this is what meant to be a human. It could go back to the neurobiology root of human brain. We human have sophisicated brain and mind.  


# Thinking and Reasoning 
## Logic 
Logic and AI has a long and interwined history. As the one of the first steps we human took to uncover the truth of being a human, many scientists especially logicians have came to the same conclution that what makes human unique is our ability to think and reason. Logic was developed as the main tool to support this goal. 

Dating back to 300 BC, their were major figures across many the field of philosophy, sociology, economics, and mathematics have looked to reasoning as one of the key distinctions of humankind. Our ability to take a set of propositions and to know their truth and false values. Only from this base of propositions can we begin to conclude or make logical combinations to create valid knowledge.

%connect peoplewith the otherperson not just thelist of events

[Aristotle](https://en.wikipedia.org/wiki/Aristotle) (384-322 BCE), as one of the first pioneers in developing the subject of logic, he set out to conduct the formal study of what is now known as 'formal logic'. He called humans "animal rationale", in his treatise, 

%On the Soul,  who saw humans as distinguised by their rationality, proposed the syllogism; Which was one of the first formulations of logic around 300 B.C. found in the Organon.

 In 1275, Ramon Llull, a spanish Theologian wrote the Ars Magna (Art of Finding Truth), which provided a method based in logic to produce new knowledge.
 
 %connectedback to labniz
 
Followed their footstep, Rene Descartes also came up with one of the best-known quotations in philosophy:"Cogito, ergo sum" ("I think, therefore I am"). Besides being a 'rational' philosopher, his contribution to mathematics was also of the first order, he is the inventor of Cartesian coordinate system and the founder of analytics geometry which has laid the foundation of the later developed calculus and mathematical analysis.=

%Charles Babbage 

%add the even pf rene insteadof thesummary 

[Gottfried Wilhelm Leibniz](https://en.wikipedia.org/wiki/Gottfried_Wilhelm_Leibniz) (1646-1716), along with Rene Descartes, was a major figure in the Continental Rationalism movement. His contributions to logic were perhaps the most important between Aristotle and the later extended model symbolic logic. As the inventor of calculus, and as the discoverer of the binary system. He suggested that a universal calculus of reasoning could be devised which would provide an automatic method of solution for all problems which could be expressinged in the universal language.


%add tje principle of mathematics 

Symbolic logic was not fully developed until the invention of Boolean Algebra. As the inventor of the Boolean Algebra, [George Boole](https://en.wikipedia.org/wiki/George_Boole) (1815-1864) is also a philosopher and logician who proposed that:
> No general method for the solution of questions in the theory of probabilities can be established which does not explicitly recognise, not only the special numerical bases of the science, but also those universal laws of thought which are the basis of all reasoning, and which, whatever they may be as to their essence, are at least mathematical as to their form.

After George Boole, plenty of other well-known logician and mathematicians joined the force to push the development of logic and helped to establish the unshakable status of logic as one of the most important properties of human. 

%discovery vs Inventiom

[Gottlob Frege](https://en.wikipedia.org/wiki/Gottlob_Frege) (1848-1925) %discovered predicate logic. He is well-acknowledged by many to be the father of analytic philosophy, concentrating on the philosophy of language and mathematics. His work was later brought to the center of the attention by Giuseppe Peano (1858–1932) and Bertrand Russell (1872–1970) and served as the beginning point for an enormous outpouring of work in formal logic. 

As the inventor of Turing machine and famous Turing test, [Alan Turing](https://en.wikipedia.org/wiki/Alan_Turing) is widely considered to be the father of theoretical computer science and artificial intelligence. The Turing machine that he designed is a mathematical model of computation which is capable of simulating that algorithm's logic can be constructed.

%rewrite the alanturing story and events and highly correlatedwith logic 

Claude Shannon (1916-2001) is noted for having founded information theory with a landmark paper, A Mathematical Theory of Communication, that he published in 1948. He is, perhaps, equally well known for founding digital circuit design theory in 1937, when—as a 21-year-old master's degree student at the Massachusetts Institute of Technology (MIT)—he wrote his thesis demonstrating that electrical applications of Boolean algebra could construct any logical numerical relationship

All of those people have brought logic to life and step by step helped the invention of the early computer and the birth of AI. 

## Reason Under Uncertainty 

Logic, as the proxy to human reasoning and thinking, gave us a way to reason under specific conditions, those conditions didn't match the typical type of problem a human comes into contact with in the real world. As we orient ourselves towards concrete and abstract goals we often have to settle with an incomplete picture, lacking the certainty of propositions that logic requires. So we needed a way to reason under uncertainty, this brought on the theory of probabilities. 

> The true logic of this world is to be found in theory of probability.
  - James Clark Maxwell

By definition, probabilistic reasoning is to combine the capacity of probability theory to handle uncertainty with the capacity of deductive logic to exploit structure of formal argument. Probability theory itself as the branch of mathematics could be considered as the tool that helps us reason under uncertainty. 

The start of probability really dates back to 1550 by Cardan, but wouldn't be of practical use until the correspondence of Pierre de Fermat (1607-1665) and Blaise Pascal (1623-1662) in 1654. 

Pascal was an important mathematician, helping create two major new areas of research: he wrote a significant treatise on the subject of projective geometry at the age of 16. Besides that, he also started some pioneering work on calculating machines when was still a teenager in 1642. After 3 years of effoft and 50 prototypes, he built 20 finished machines called Pascal's calculators, eastablishing him as one of the first 2 inventors ofthe machanical calculator. 

Together with René Descartes, Fermat was one of the two leading mathematicians of the first half of the 17th century. He has pioneered the work in determining maxima, minima, and tangents to various curves that was equivalent to differential calculus in his work Methodus ad disquirendam maximam et minimam and in De tangentibus linearum curvarum published in 1636.
 
Pascal and Fermet were discussing the problem that Chevalier de Méré proposed to Pascal, titled the problem of points. 

The game went as follows:

Suppose two players who have equal chance of wining each round. Their is a prize pot where they have both contributed 50% each. The game has a priori number of rounds and who every wins the most rounds takes the prize pot. But let us suppose that the game is interupptd before the end of the game. How does one divide the pot fairly?

There were others how attempted to solve this problem but the current outcomes lead to edge cases that were unintuitive and were mostly based on the past games.

Pascal and Ferment made a breakthrough by considering (from the interruption forward) the possible futures that might be possible if the players had continued until the end. 

$$ r = rounds\ for\ p1\ to\ win $$
$$ s = rounds\ for\ p2\ to\ win $$
 $$ r + s - 1 $$
 
 $$ Number\ of\ possible\ outcomes\ 2^{r+s-1} $$ 
 Odds vs. expectations
 Soon after Pascale improving on Ferment's results, Christiaan Huygens, used Pascal's result to create the first treatise of modern Probability based on expectation.s
 
> For example, the probability of throwing a 6 on a die twice is 1⁄6 x 1⁄6 = 1⁄36 ("and" works like multiplication); the probability of throwing either a 3 or a 6 is 1⁄6 + 1⁄6 = 1⁄3 ("or" works like addition).
 
 This formalisation would lead to the foundation of modern probability. 
 
 The work done by Fermat and Pascal into the calculus of probabilities laid important groundwork for Leibniz' formulation of the calculus. They are now regarded as joint founders of probability theory.
 
  Later Pascal would go on to use probability in his most notable work, Pensees, used a probabilistic argument, Pascal's Wager, to justify belief in God and a virtuous life. 
 
 Around the same time, Christiaan Huygens (1629-1695) also started his work in probability and wrote his first treatise on probability theory in 1657 with the work Van Rekeningh in Spelen van Gluck. Frans van Schooten translated the work as De ratiociniis in ludo aleae ("On Reasoning in Games of Chance"). The work is a systematic treatise on probability and deals with games of chance and in particular the problem of points.
 
 The modern concept of probability grew out of the use of expectation values by Huygens and Blaise Pascal.  
 
 These probability pioneers has established the method that is now called *classical approach* to compute probabilities.
 
A few decades later Jacob Bernoulli (1654-1705) proved that the frequency method and the classical method are consistent with one another in his book Ars Conjectandi in 1713. His significant contributions to probability also include the derivation of the first version of the *law of large numbers*  in his work Ars Conjectandi and the start of some early work on the probability inference. 
 
 However, *classical approach* is limited by its assumption that the possibility of all outcomes of an event has to be equal.

 Nearly a centruy later, following the work of Fermet, Pascal and Huygens, Abraham de Moivre (1667-1754) gave the first statement of the formula for the normal distribution. De Moivre provided many tools to make the classical method more useful, including the multiplication rule, in his book The Doctrine of Chances in 1718 which is said to have been prized by gamblers. De Moivre first discovered Binet's formula, the closed-form expression for Fibonacci numbers linking the nth power of the golden ratio φ to the nth Fibonacci number. He also was the first to postulate the central limit theorem, a cornerstone of probability theory.

By then, the probability has gradually moved from games of chance to a scientific subject. A new era of probability was began from 18th century. 

As one of the leading figures in formulating Bayes' Theorem, Thomas Bayes (1701-1761) proposed the unique solution to a problem of inverse probability that was presented in "An Essay towards solving a Problem in the Doctrine of Chances" which was read to the Royal Society in 1763 after Bayes' death. This specific case of the probability theorem bears his name: Bayes' Theorem, however, Bayes himself may not have embraced the broad interpretation now called Bayesian, which was in fact pioneered and popularized by Pierre-Simon Laplace. 

As mentioned earlier, the Bayesian interpretation of probability was developed mainly by Laplace. Pierre-Simon Laplace (1749-1827) presented a mathematical theory of probability with an emphasis on scientific applications in his 1812 book Theorie Analytique des Probabilities, where he elucidates the use of probablity theory to scientific and practical problems in statistical mechanics, actuarial mathematics, error, and so on.

However, Laplace has only considered the classical method, leaving no indication on how the method was to be applied to general problems. By 1850, many mathematicians were attempting to refine probability in terms of frequency methods. 

Around that time, Andrey Markov (1856-1922) introduced the Markov chains in 1906 when he produced the first theoretical results for stochastic processes by using the term “chain” for the first time. In 1913 he calculated letter sequences of the Russian language. 
 
 A generalization to countable infinite state spaces was given by Andrey Kolmogorov (1931). Markov chains are related to Brownian motion and the ergodic hypothesis, two topics in physics which were important in the early years of the twentieth century. But Markov appears to have pursued this out of a mathematical motivation, namely the extension of the law of large numbers to dependent events. Out of this approach grew a general stw a general statistical instrument, the so-called stochastic Markov process. 

Kolmogorov also developed the first rigorous approach to probability in his 1933 monograph Grundbegriffe der Wahrscheinlichkeitsrechnun. His pioneering work, About the Analytical Methods of Probability Theory, was published (in German) in 1931. In 1933, Kolmogorov published his book, Foundations of the Theory of Probability, in which he combined the notion of sample space, introduced by Richard von Mises, and measure theory and presented his axiom system for probability theory in his book. This became the mostly undisputed axiomatic basis for modern probability theory.

R. A. Fisher (1890–1962), as one of the most important statistcian in history, also made some great contribution to probability. He recommended, analyzed and made the Maximum-likelihood popular between 1912 and 1922, although it had been used earlier by Gauss, Laplace. Fisher's 1924 article On a distribution yielding the error functions of several well known statistics developed Fisher's z-distribution a new statistical method, commonly used decades later as the F distribution. In his 1937 paper The wave of advance of advantageous genes he proposed Fisher's equation in the context of population dynamics to describe the spatial spread of an advantageous allele and explored its travelling wave solutions. Out of this also came the Fisher–Kolmogorov equation.

During this period of time, probability has also been widly adopted and further developed by many scientist in other domains i.e. Physicists. Some of those leading figures include Josiah Willard Gibbs and E.T. Jaynes.

Josiah Willard Gibbs (1839-1903) together with James Clerk Maxwell and Ludwig Boltzmann created statistical mechanics (a term that he coined), explaining the laws of thermodynamics as consequences of the statistical properties of ensembles of the possible states of a physical system composed of many particles. Gibbs's derivation of the laws of thermodynamics from the statistical properties of systems consisting of many particles was presented in his highly influential textbook Elementary Principles in Statistical Mechanics, published in 1902.

Following the footsteps of Gibbs, E.T. Jaynes (1922-1998) wrote extensively on statistical mechanics and on foundations of probability and statistical inference. He initiated the maximum entropy interpretation of thermodynamics as being a particular application of more general Bayesian/information theory techniques in 1957. Jaynes also strongly promoted the interpretation of probability theory as an extension of logic. His posthumous book, Probability Theory: The Logic of Science (2003) gathers various threads of modern thinking about Bayesian probability and statistical inference, develops the notion of probability theory as extended logic, and contrasts the advantages of Bayesian techniques with the results of other approaches. 

Speaking about Bayesian inference, Judea Pearl (1936-) is best known for developing a theory of causal and counterfactual inference based on structural models. He is also known for championing the probabilistic approach to artificial intelligence and the development of Bayesian networks. He is the 2011 winner of the ACM Turing Award, the highest distinction in computer science, "for fundamental contributions to artificial intelligence through the development of a calculus for probabilistic and causal reasoning". His book Probabilistic Reasoning in Intelligent Systems published in 1988 is a complete and accessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. 

 Probability theory was initially derived from the gambling puzzles and has graduadully evolved to be the most powerful tool for inference. It is still debatable whether probability theory should be considered as extended logic. It is clear for us that most handy tools to help us reason under uncertainty. Together with logic, probability is part of the fomal study of reasoning.
 
 
 
 ##Symbolic AI and Expert System
 
 As mentioned at the beginning, one of the way humans are intelligent is our ability to manipulate symbols i.e. language and hence it also became the way we communicate with each other. 
 
 
 As for the term Symbolic Artificial Intelligence which is also known as "Good Old-Fashioned Artificial Intelligence(GOFAI)" is given by John Haugeland  in his 1985 book Artificial Intelligence.
 
 > The Very Idea, which explored the philosophical implications of artificial intelligence research. In robotics the analogous term is GOFR ("Good Old-Fashioned Robotics").
 
  Symbolic was once the dominant paradigm in AI community from the post-War era until the late 1980s. We will discuss the turn after the symbolic AI more in depth in the later chapters.
 
 Symbolic AI is made to make use of strings and symbols to represent real-world enetities or concepts. These strings are often manually or incrementally stored in a knowledge base and made available to the interfacing human/machine when requested. It is especially useful when used to make intelligent conclusions and decisions based on the memorized facts and rules put together by propositional logic or first-order predicate calculus techniques. Implementations of symbolic reasoning are called rules engines or Expert systems or Knowledge graphs. 
 
 In constrast to symbolic AI, non-symbolic AI is a system that is designed to not rely on the symbols and signs but given just the ability to learn from the empirical data. 
 

 One of the most famous projects in designing and implemening symbolic is Cyc. Douglas Lenat began the project CYC in July 1984 at MCC, and it is the world's longest-lived artificial intelligence project, attempting to assemble a comprehensive ontology and knowledge base that spans the basic concepts and "rules of thumb" about how the world works (think common sense knowledge but focusing more on things that rarely get written down or said, in contrast with facts one might find somewhere on the internet or retrieve via a search engine or Wikipedia), with the goal of enabling AI applications to perform human-like reasoning and be less "brittle" when confronted with novel situations that were not preconceived.
 

Follow the simmilar blueprint, Google also made a gaint knowledge graph for their search system which is used to provide the information in the top box under your query when you search. What these systems shared in common is that they are essentially piles of nested if-then statements attempting to draw concolution about entities and their relations. 
 

 
However, symbolic AI has long and worth mentioning history before it got popularized in 1980s.

%A von Neumann architecture machine, designed by physicist and mathematician John von Neumann (1903–1957) is a theoretical design for a stored program computer that serves as the basis for almost all modern computers.


In 1959, Newell, Shaw and Simon developed a system called General Problem Solver (GPS), which is a summary of the thinking activity for people to solve problems. GPS was the first computer program in history which is able to separate its knowledge of problems (rules represented as input data) from its strategy of how to solve problems (a generic solver engine). It has set an promising example for the later symbolic AI projects. 


%As mentioned earlier, Expert systems is also considered as a brach of Symbolic AI. It ws introduced around 1965 by the Stanford Heuristic Programming Project led by Edward Feigenbaum,
An expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.

%Since then, MIT has developed the MACSYMA system [3], as a mathematician's assistant, which uses heuristics to transform algebraic expressions. After continuous expansion, it can solve more than 600 kinds of mathematical problems, including calculus, matrix operation, solving equation, etc. The successful development of these systems makes the expert system widely concerned by academia and engineering

Later in 1968, MIT has also developed a symbolic manipulator system called Macsyma which stands for MAC's SYmbolic MAnipulator. As a mathematician's assistant, it uses heuristics to transform algebraic expressions. After continuous expansion, it can solve more than 600 kinds of mathematical problems, including calculus, matrix operation, solving equation, etc. The successful development of these systems makes the expert system widely concerned by academia and engineering. In 1982, Macsyma was licensed to Symbolics and became a commercial product. In 1992, Symbolics Macsyma was spun off to Macsyma, Inc., which continued to develop Macsyma until 1999. That version is still available for Microsoft's Windows XP operating system.




Expert systems were introduced around 1965[11] by the Stanford Heuristic Programming Project led by Edward Feigenbaum,



 Allen Newell, Cliff Shaw, and Herbert Simon’s, Logic Theorist. The Logic Theorist was a program designed to mimic the problem solving skills of a human and was funded by Research and Development (RAND) Corporation.


**Symbolic AI vs Deep Learning **
When talking about AI many people will natually think of deep learning when they hope for the future. However, there are some obvious flaws of deep learning such as its lack of model interpretability and the amount of data that deep neural networks require so as to learn. 

Symbolic AI, as one of the other branches of AI, provided some hints on how to solve these issues. Research in deep symbolic learning has shown to enable deep neural network to manipulate, generate and otherwise cohabitate with concepts expressed in strings of characters, could extend its explainability. 

 By feeing it human-readable information related to what you think and communicate, Symbolic AI provides us a powerful and useful framework to design an intelligent system. 
